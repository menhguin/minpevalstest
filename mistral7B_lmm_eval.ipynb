{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/menhguin/minpevalstest/blob/main/mistral7B_lmm_eval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM Evaluation\n",
        "\n",
        "In this notebook, we'll use the [language model evaluation harness](https://github.com/EleutherAI/lm-evaluation-harness)\n",
        "utility built by EleutherAI to evaluate our model on a suite of different tasks."
      ],
      "metadata": {
        "id": "cFLcmAxK7aYW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lm_eval[wandb]"
      ],
      "metadata": {
        "id": "n2HciuARkWBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Re0eppYizJ1"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!git clone https://github.com/EleutherAI/lm-evaluation-harness\n",
        "!pip install -e lm-evaluation-harness"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lm-eval[vllm]"
      ],
      "metadata": {
        "id": "XR_o5KiAa2mc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/lm-evaluation-harness/lm_eval \\\n",
        "    --model vllm \\\n",
        "    --model_args pretrained=mistralai/Mistral-7B-v0.1,dtype=\"float\", \\\n",
        "    --batch_size 4 \\\n",
        "    --tasks gsm8k \\\n",
        "    --num_fewshot 5 \\\n",
        "    --wandb_args project=lm-eval-harness-integration \\\n",
        "    --log_samples \\\n",
        "    --output_path ./lm-eval-output/ \\\n",
        "    --gen_kwargs min_p=\"0.05\" \\\n",
        "    --device cuda"
      ],
      "metadata": {
        "id": "mTSKBJlVjaB-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}